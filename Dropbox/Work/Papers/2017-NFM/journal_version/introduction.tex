\section{Introduction}\label{sec:introduction}

Over the last decade, machine learning (ML) algorithms have achieved impressive results providing solutions to practical large-scale
problems (see, e.g.,~\cite{blum1997selection,michalski2013machine,jia2014caffe,hinton2012deep}).
Not surprisingly, ML is being used in 
cyber-physical systems (CPS) --- systems that are integrations of
computation with physical processes. 
For example, semi-autonomous vehicles employ Adaptive Cruise Controllers (ACC)
or Lane Keeping Assist Systems (LKAS) that rely heavily on image classifiers
providing input to the software controlling electric and mechanical 
subsystems (see, e.g.,~\cite{nvidiaself-arxiv16}).
The safety-critical nature of such systems involving ML
raises the need for formal methods~\cite{SeshiaS16}.
In particular, how do we systematically find bugs in such systems?

We formulate this question as the falsification problem for CPS models with 
ML components (CPSML): given a formal specification $\varphi$ 
(say in a formalism such as signal temporal logic~\cite{maler2004monitoring}) 
and a CPSML model $M$, find an input for which $M$ does {\it not} satisfy $\varphi$. A falsifying 
input generates a counterexample trace that reveals a bug.
To solve this problem, multiple challenges must be tackled. 
First, the input space to be searched can be intractable.
For instance, a simple model of a semi-autonomous car already involves
several control signals (e.g., the angle of the acceleration pedal, steering angle) 
and other rich sensor input (e.g., images captured by a camera, LIDAR, RADAR). 
Second, CPSML are often designed using languages 
(such as C, C++, or Simulink), for which clear semantics are not given,
and involve third-party components that are opaque or poorly-specified. 
This obstructs the development of formal methods for the analysis 
of CPSML models and may force one 
to treat them as gray-boxes or black-boxes.
Third, the formal verification of ML components is a difficult,
and somewhat ill-posed problem due to
the complexity of the underlying ML algorithms,
large feature spaces, and the lack of consensus on a formal definition
of correctness (see~\cite{SeshiaS16} for a longer discussion). 
Hence, we need a technique 
to systematically analyze ML components within the context of a CPS.

In this paper, we propose a new framework for the falsification of CPSML
addressing the issues described above. 
Our technique is compositional (modular) in that it divides the 
search space for falsification
into that of the ML component and of the
remainder of the system, while 
establishing a connection between the two.
The obtained subspaces are respectively analyzed by 
a temporal logic falsifier (``CPS Analyzer'') and a machine learning
analyzer (``ML analyzer'') that cooperate to search for
a behavior of the closed-loop system that violates the property $\varphi$.
This cooperation mainly comprises a sequence of input space projections,
passing information about interesting regions in the input space
of the full CPSML model to identify a sub-space of the input space
of the ML component. The resulting projected input space of the ML
component is typically smaller than the full input space. Moreover,
misclassifications in this space can be mapped back to smaller subsets
of the CPSML input space in which 
counterexamples are easier to find.
Importantly, our approach can handle {\em any machine learning technique},
including the methods based on deep neural networks~\cite{hinton2012deep} 
that have proved effective in many recent applications.
The proposed ML Analyzer is a tool that analyzes the
input space for the ML classifier and determines
a region of the input space that could be relevant for the full 
cyber-physical system's correctness. More concretely,
the analyzer identifies sets of misclassifying
features, i.e., inputs that ``fool'' the ML algorithm. The analysis 
is performed by considering subsets of parameterized features spaces
that are used to approximate the ML components by simpler functions. 
The information gathered by the temporal logic falsifier and the
ML analyzer together reduce the search space,
providing an efficient approach to falsification for CPSML models.

\begin{figure}
  	\begin{center}
    	%\includegraphics[width=0.5\textwidth]{./pics/aebs_ex.png}
		\includegraphics[width=0.8\textwidth]{./pics/CPSML-closed_loop.pdf}
	 \end{center}
	\caption{Automatic Emergency Braking System (AEBS) in closed loop. A machine learning based image classifier is used to perceive objects in the ego vehicle's frame of view.\label{fig:aebs_ex}}
\end{figure}


\begin{example}\label{ex:aebs}

As an illustrative example, let us consider a simple model of an Automatic Emergency Braking System (AEBS),
that attempts to detect objects in front of a vehicle and actuate
the brakes when needed to avert a collision.
Figure~\ref{fig:aebs_ex} shows the AEBS as a system
composed of a controller (automatic braking), a plant (vehicle sub-system under control, including transmission), and an advanced sensor (camera along
with an obstacle detector based on deep learning). The AEBS, when combined
with the vehicle's environment, forms a closed loop control system.
The controller regulates the acceleration and braking of the plant using the velocity of
the subject (ego) vehicle and the distance between it and an obstacle.
The sensor used to detect the obstacle includes a camera along with an image classifier based on deep neural networks.
In general, this sensor can provide noisy measurements due to incorrect image 
classifications which in turn can affect the correctness of the overall system.

Suppose we want to verify 
whether the distance between the subject vehicle and a preceding obstacle is always larger than
$5$ meters. Such a verification requires the exploration of a very 
large input space comprising 
the control inputs (e.g., acceleration and braking pedal angles) and the ML component's feature space
(e.g., all the possible pictures observable by the camera). 
The latter space is particularly large --- for example,
note that the feature space of RGB images of dimension 
$1000\times600$px (for an image classifier) 
contains $256^{1000\times 600 \times 3}$ elements.
\end{example}

At first, the input space of the model described in Example~\ref{ex:aebs} appears intractably large.
However, the twin notions of {\em abstraction} and {\em compositionality},
central to much of the advances in formal verification, can help address
this challenge. As mentioned earlier, we decompose the overall CPSML
model input space into two parts: (i) the input space of the ML component,
and (ii) the input space for the rest of the system -- i.e., the CPSML
model with an abstraction of the ML component.
A {\em CPS Analyzer} operates on the latter ``pure CPS'' input space,
while an {\em ML Analyzer} handles the former. 
The two analyzers communicate information as follows:
\begin{enumerate}
	\item The CPS Analyzer initially performs conservative analyses assuming abstractions of the ML component. In particular, consider two extreme abstractions --- a \lq\lq perfect ML classifier\rq\rq\ (i.e., all feature vectors are correctly classified), and a ``completely-wrong ML classifier'' (all feature vectors are misclassified). Abstraction permits the CPS Analyzer to operate on a lower-dimensional input space (the ``pure CPS'' one) and identify a region in this space that may be
affected by the malfunctioning of some ML modules -- a so-called ``region of interest'' or ``region of uncertainty.''
This region is communicated to the ML Analyzer.

	\item The ML Analyzer projects the region of uncertainty (ROU) onto its input space, and performs a detailed analysis of that input sub-space. Since this detailed analysis uses only the ML classifier (not the full CPSML model), it is a more tractable problem. In this paper, we present a novel
sampling-based approach to explore the input sub-space for the ML
component. We can also leverage other advances in analysis of machine 
learning systems operating on rich sensor inputs and for applications
such as autonomous driving (see the related work section that follows).

	\item When the ML Analyzer finds interesting test cases (e.g., those that trigger misclassifications of inputs whose labels are easily inferred), it communicates that information back to the CPS Analyzer, which checks whether the ML misclassification can lead to a system-level safety violation (e.g., a collision). If yes, we have found a system-level counterexample.
If not, the ROU is updated and the revised ROU passed back to the ML Analyzer. 

\end{enumerate}
The communication between the CPS Analyzer and ML Analyzer continues
until either we find a system-level counterexample, or we run out of
resources. For the class of CPSML models we consider, including those
with highly non-linear dynamics and even black-box components, one
cannot expect to prove system correctness.
We focus on specifications in Signal Temporal Logic (STL), and
for this reason use a temporal logic falsifier, Breach~\cite{donze2010breach}, 
as our CPS Analyzer.
Even though temporal logic falsification is a mature technology with
initial industrial adoption (e.g.,~\cite{yamaguchi-fmcad16}),
several technical challenges remain.
First, we need to construct the validity domain of an STL
specification --- the input sub-space where the property is 
satisfied --- for a CPSML model with abstracted (correct/incorrect) 
ML components, and identify the region of uncertainty (ROU).
Second, we need a method to relate the ROU 
to the feature space of the ML modules. 
Third, we need to systematically analyze the feature space of 
the ML component
with the goal of finding feature vectors leading to misclassifications.
We describe in detail in Sections~\ref{sec:framework} 
and~\ref{sec:MLanalysis} how we tackle these challenges.


%%% old stuff commented out
\comment{
However, we can observe some interesting aspects of the relationship between the ``pure CPS'' input space
and its ML feature space:
\begin{enumerate}
	\item Under the assumption of ``perfect ML components'' (i.e., all feature vectors are correctly classified),
		we can study the CPSML model on a 
		lower-dimensional input space (the ``pure CPS'' one) and identify regions of values that satisfy the specification but might be
		affected by the malfunctioning of some ML modules;
	\item Instead of verifying the ML components on their whole feature spaces, we can focus only on those
		features related to the non-robust input values identified in the previous step, and
	\item If we are able to determine misclassifications on the restricted feature space,
		then we can relate them back to CPSML input space, thus focusing the falsification on a smaller
		input space.
\end{enumerate}
} % end comment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In summary, the main contributions of this paper are:
\begin{myitemize}

\item A compositional framework for
the falsification of temporal logic properties of arbitrary 
CPSML models that works for any machine learning classifier.

\item A machine learning analyzer that identifies
misclassifications leading to system-level property violations, 
based on two main ideas:
\begin{myitemize}
	\item[-] An input space parameterization used to abstract the feature space of the ML component and relate it
		to the CPSML input space, and
	\item[-] A classifier approximation method used to abstract
the ML component and identify misclassifications that can 
lead to executions of the CPSML that violate the temporal logic specification.
\end{myitemize}

\item
An experimental demonstration of the effectiveness of our approach 
on two instantiations of the Automatic Emergency Braking System (AEBS)
example with multiple deep neural networks trained for object
detection and classification, including some developed by experts
in the machine learning and computer vision communities.
\end{myitemize}
In Sec.~\ref{sec:experiments}, we give detailed experimental results
on an Automatic Emergency Braking System (AEBS)
involving an image classifier for obstacle detection
based on deep neural networks developed and trained
using leading software packages --- 
AlexNet developed with Caffe~\cite{jia2014caffe} 
and Inception-v3 developed with TensorFlow~\cite{tensorflow2015}. 
In this journal version of our original conference paper~\cite{dreossi-nfm17},
we also present a new case study, an AEBS deployed within 
the Udacity self-driving car simulator~\cite{udacitySim-www} trained
on images generated from the simulator.


\subsection*{Related Work}

The verification of both CPS and ML algorithms have attracted several research efforts,
and we focus here on the most closely related work.
Techniques for the falsification of temporal logic specifications against CPS models
have been implemented based on nonlinear optimization methods and stochastic search strategies
(e.g., Breach~\cite{donze2010breach}, S-TaLiRo~\cite{annpureddyLFS11}, 
RRT-REX~\cite{dreossi2015efficient}, C2E2~\cite{duggirala2015c2e2}).
While the verification of ML programs is less well-defined~\cite{SeshiaS16},
recent efforts~\cite{szegedy2013intriguing} show how
even well trained neural networks can be sensitive to small adversarial perturbations, i.e., small intentional modifications that
lead the network to misclassify the altered input with large confidence.
Other efforts have tried to characterize the correctness of neural networks 
in terms of risk~\cite{vapnik1991principles}
(i.e., probability of misclassifying a given input) or 
robustness~\cite{fawzi2015analysis,carlini-ieeesp17} 
(i.e., a minimal perturbation leading to a misclassification),
while others proposed methods to generate pictures~\cite{nguyen2015deep,dreossi-rmlw17} 
or perturbations~\cite{moosavi2015deepfool,nguyen2015deep,huang-arxiv16}
including methods based on satisfiability modulo theories (SMT)~\cite{katz-cav17}
in such a way to ``fool'' neural networks. 
These methods, while very promising, are mostly limited
to analyzing the ML components in isolation, and not in the context of a complex, closed-loop
cyber-physical system. 
To the best of our knowledge, 
our work is the first to address the verification of temporal logic
properties of CPSML---the combination
of CPS and ML systems.
The work that is closest in spirit to ours is that on DeepXplore~\cite{pei-sosp17},
where the authors present a whitebox software testing approach
for deep learning systems. However, there are some important
differences: their work performs a detailed analysis of the learning software, whereas
ours analyzes the entire closed-loop CPS while delegating the software
analysis to the machine learning analyzer. Further, we consider temporal
logic falsification whereas their work uses software and neural
network coverage metrics. It may be interesting to see how these
approaches can be combined.



